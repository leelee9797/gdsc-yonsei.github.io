---
published : true
title: "[ML Research]á„á…¬á„Œá…¥á†¨á„’á…ª á„‹á…¡á†¯á„€á…©á„…á…µá„Œá…³á†·(Optimizer)"
date: 2021-04-06
excerpt: "ìµœì í™” ì•Œê³ ë¦¬ì¦˜ Optimizerì— ëŒ€í•œ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤."
use_math: true
categories:
  - MLResearch
author : SeBin Oh
---

<br/>

ì•ˆë…•í•˜ì„¸ìš”~ ì—¬ëŸ¬ë¶„!  
ë‹¤ë“¤ ë°˜ê°€ì›Œìš”. ğŸ§šâ€â™€ï¸

<br/>

ì˜¤ëŠ˜ì€ **ìµœì í™” ì•Œê³ ë¦¬ì¦˜ Optimizer** ì— ëŒ€í•´ ì•Œì•„ë³´ë ¤ê³  í•©ë‹ˆë‹¤!

<br/>

ëª©ì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<br/>

### ğŸ“– ëª©ì°¨

- [ğŸ›¤  ìµœì í™” ì•Œê³ ë¦¬ì¦˜(Optimizer)ì´ë€?](#--ìµœì í™”-ì•Œê³ ë¦¬ì¦˜optimizerì´ë€)
- [1. ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent: GD)](#1-ê²½ì‚¬í•˜ê°•ë²•gradient-descent-gd)
- [2. í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(Stochastic Gradient Descent: SGD)](#2-í™•ë¥ ì -ê²½ì‚¬í•˜ê°•ë²•stochastic-gradient-descent-sgd)
- [3. ë¯¸ë‹ˆ ë°°ì¹˜ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(Mini-Batch Gradient Descent)](#3-ë¯¸ë‹ˆ-ë°°ì¹˜-í™•ë¥ ì -ê²½ì‚¬í•˜ê°•ë²•mini-batch-gradient-descent)
- [4. ê´€ì„±(Momentum)](#4-ê´€ì„±momentum)
- [5. Nesterov Accelerated Gradient(NAG)](#5-nesterov-accelerated-gradientnag)
- [6\. Adaptive Gradient(Adagrad)](#6-adaptive-gradientadagrad)
- [7\. Root Mean Square PROPagation(RMSprop)](#7-root-mean-square-propagationrmsprop)
- [8. Adaptive Delta(AdaDelta)](#8-adaptive-deltaadadelta)
- [9. Adaptive moment esimation(Adam)](#9-adaptive-moment-esimationadam)
  - [ğŸ¤ ë§ˆì¹˜ë©°](#-ë§ˆì¹˜ë©°)
  - [Reference](#reference)
  - [ê¸€ì“´ì´](#ê¸€ì“´ì´)

<br/>

ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ë€ ë¬´ì—‡ì¸ê°€! í•˜ë‚˜í•˜ë‚˜ ì‚´í´ë³´ì‹œì£ . 

<br/>

## ğŸ›¤  ìµœì í™” ì•Œê³ ë¦¬ì¦˜(Optimizer)ì´ë€?  

<br/>

ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ë€ ì‹ ê²½ë§(neural network) í•™ìŠµì—ì„œ ì†ì‹¤í•¨ìˆ˜(loss function) ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ë§¤ê°œë³€ìˆ˜, neural networkì˜ ê°€ì¤‘ì¹˜ì™€ í•™ìŠµë¥ ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  
ì£¼ë¡œ Gradient Descent Algorithmë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ SGDì—ì„œ ë³€í˜•ëœ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ìµœì í™”í•¨ìˆ˜ê°€ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FuXBgR%2FbtqVIJwz6Vb%2FFqut2qOU32qSyZ9l87Xp8k%2Fimg.png)

<br/>

ì•„ë˜ëŠ” ëŒ€í‘œì ì¸ Optimizer ê¸°ë²•ë“¤ì´ ìµœì ê°’ì„ ì°¾ì•„ê°€ëŠ” ê·¸ë˜í”„ë¡œ ê°ê°ì˜ íŠ¹ì„±ì´ ì˜ ë“œëŸ¬ë‚˜ ìˆìŠµë‹ˆë‹¤.

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FNXkd0%2FbtqVK8P5Qv3%2F8WmnkPUWNTsPFFKPLGpQvK%2Fimg.gif)  

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbD5etd%2FbtqVTHp8mxY%2FU7lXvhVmWPLk1ShaSW0lvk%2Fimg.gif)  

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FupNYI%2FbtqVSvwJGgh%2FI7lxtFog1r2ommkO9uROKK%2Fimg.gif)  

<br/>

## 1. ê²½ì‚¬í•˜ê°•ë²•(Gradient Descent: GD)

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FnhTKN%2FbtqVTG5QSUI%2FIEHqwDs7UoXh3reS749bZk%2Fimg.png)

<br/>

ê°€ì¥ ê¸°ë³¸ì ì´ë©° ë§ì´ ì“°ì´ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì „ì²´ ë°ì´í„°ì— ëŒ€í•˜ì—¬ ì†ì‹¤í•¨ìˆ˜(loss function)ì˜ í˜„ ê°€ì¤‘ì¹˜ì—ì„œì˜ ê¸°ìš¸ê¸°(gradient)ë¥¼ êµ¬í•´ì„œ ì†ì‹¤(loss)ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ local minimaì— ë¹ ì§ˆ ìˆ˜ ìˆìœ¼ë©°, ì „ì²´ ë°ì´í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ë§ì€ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ì´ ì†Œìš”ëœë‹¤ëŠ” ë‹¨ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•˜ì—¬ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)ì´ ë“±ì¥í•˜ì˜€ìŠµë‹ˆë‹¤.

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2F073hY%2FbtqVK7XZHPA%2F6KvtSJjKj4xG4oTSK3EOIk%2Fimg.png)

<br/>

## 2. í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(Stochastic Gradient Descent: SGD)

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FxgXbN%2FbtqVSuLohtA%2FhNisa6ooPgxcNP5nBeEX20%2Fimg.png)

<br/>

í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)ëŠ” ë§¤ê°œë³€ìˆ˜ ê°’ì„ ì¡°ì • ì‹œ ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹ˆë¼ ëœë¤ìœ¼ë¡œ ì„ íƒí•œ í•˜ë‚˜ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•˜ëŠ” optimizerì…ë‹ˆë‹¤. ì†ì‹¤í•¨ìˆ˜(loss function)ë¥¼ ê³„ì‚°í•  ë•Œ ì „ì²´ ë°ì´í„°ê°€ ì•„ë‹Œ mini-batch í¬ê¸°ë¥¼ ê²°ì •í•˜ì—¬ ê·¸ í¬ê¸°ì˜ ë°ì´í„°ë§ˆë‹¤ ê¸°ìš¸ê¸°(gradient)ë¥¼ êµ¬í•´ì„œ ì†ì‹¤(loss)ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ëª¨ë¸ parametersë¥¼ ë¹ ë¥´ê²Œ ì°¾ê¸° ë•Œë¬¸ì— ìˆ˜ë ´í•˜ëŠ”ë° ì ì€ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. ê·¸ë˜ì„œ GDë³´ë‹¤ ìƒëŒ€ì ìœ¼ë¡œ ì²˜ë¦¬ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë§¤ í•™ìŠµë§ˆë‹¤ parametersë¥¼ ì°¾ëŠ”ë° ê¸°ìš¸ê¸°(gradient) í¬ê¸°ì˜ ë³€ë™ í­ì´ ì»¤ì„œ ë¶„ì‚°ì´ í½ë‹ˆë‹¤. ë¶„ì‚°ì´ í° ê¸°ìš¸ê¸°(gradient)ëŠ” í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)ê°€ local minimumì—ì„œ ë¹ ì ¸ë‚˜ì˜¤ëŠ”ë° ë„ì›€ì´ ë˜ëŠ” ë™ì‹œì— ìˆ˜ë ´ì„ ë°©í•´í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FqzZr5%2FbtqVSt6JUaF%2FJ7RyVwKKsMWeHWKngr1uzK%2Fimg.png)

<br/>

## 3. ë¯¸ë‹ˆ ë°°ì¹˜ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(Mini-Batch Gradient Descent)

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FzyXwx%2FbtqVJSGOJZn%2Ffiwd3NwKsvWp7z6ns22GWk%2Fimg.png)

<br/>

ë¯¸ë‹ˆ ë°°ì¹˜ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(Mini-Batch Gradient Descent)ëŠ” ê²½ì‚¬í•˜ê°•ë²•(GD)ê³¼ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)ì˜ ì¥ì ë§Œ ëª¨ì•„ì„œ ë§Œë“  optimizerì…ë‹ˆë‹¤. ë§¤ ë°°ì¹˜ í›„ì— ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë°ì´í„° ì„¸íŠ¸ëŠ” ë‹¤ì–‘í•œ ë°°ì¹˜ë¡œ ë¶„í• ë˜ê³  ë§¤ ë°°ì¹˜ í›„ì— ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ëª¨ë¸ ë³€ìˆ˜ë“¤ì„ ìì£¼ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆê³  ì ì€ ë¶„ì‚°ì„ ê°€ì§€ë©° ì¤‘ê°„ í¬ê¸°ì˜ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í•™ìŠµë¥ (learning rate)ì´ ê¸°ìš¸ê¸°(gradient)ì— ë¹„í•´ ì‘ì„ ê²½ìš°, ìˆ˜ë ´ì— ë§ì€ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ëª¨ë“  ë³€ìˆ˜ë“¤ì€ ê³ ì •ëœ í•™ìŠµë¥ (learning rate)ì„ ê°€ì§€ê³  ìˆì–´ local minimumì— ë¹ ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br/>

## 4. ê´€ì„±(Momentum)

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdDvQJx%2FbtqVJQWxxsf%2F377kLhqrzxhorRMKXNoMl0%2Fimg.png)

<br/>

Momentumì€ ê´€ì„±ì„ ì‘ìš©í•œ ë°©ë²•ìœ¼ë¡œ, SGDì—ì„œ ì¶”ì§„ë ¥, ì—¬ì„¸, íƒ€ì„± ë“± ë¬¼ì²´ê°€ í•œ ë°©í–¥ìœ¼ë¡œ ì§€ì†ì ìœ¼ë¡œ ë³€í™”í•˜ë ¤ëŠ” íŠ¹ì§•ì„ ì´ìš©í•˜ì—¬ ë§Œë“  optimizerì…ë‹ˆë‹¤. ì¦‰, ì´ì „ í•™ìŠµì—ì„œì˜ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ì— ê´€ì„±ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Momentumì€ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•(SGD)ì—ì„œ ê³„ì‚°ëœ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°(gradient)ì— í•œ ì‹œì (step) ì „ì˜ ì ‘ì„ ì˜ ê¸°ìš¸ê¸°(gradient) ê°’ì„ ì¼ì •í•œ ë¹„ìœ¨ë§Œí¼ ë°˜ì˜í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ì˜ˆë¥¼ ë“¤ë©´ ì–¸ë•ì—ì„œ ê³µì´ ë‚´ë ¤ì˜¬ ë•Œ, ì¤‘ê°„ì— ì‘ì€ ì›…ë©ì´ì— ë¹ ì§€ë”ë¼ë„ ê´€ì„±ì˜ í˜ìœ¼ë¡œ ë„˜ì–´ì„œëŠ” íš¨ê³¼ë¥¼ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FRLlh7%2FbtqVRwbzD0C%2FahSHRpTio5I24KWdLfmqa1%2Fimg.png)

<br/>

local minimumì— ë„ë‹¬í•˜ì˜€ì„ ë•Œ, ê¸°ìš¸ê¸°(gradient)ê°€ 0ì´ë¼ì„œ ê¸°ì¡´ì˜ ê²½ì‚¬ í•˜ê°•ë²•(GD)ì´ë¼ë©´ ì´ë¥¼ global minimumìœ¼ë¡œ ì˜ëª» ì¸ì‹í•˜ì—¬ ê³„ì‚°ì´ ëë‚¬ì„ ìˆ˜ë„ ìˆì§€ë§Œ, ê´€ì„±ì„ ì ìš©í•˜ë©´ ê°’ì´ ì¡°ì ˆë˜ë©´ì„œ local minimumì—ì„œ íƒˆì¶œí•˜ëŠ” íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FecGCty%2FbtqVQV3IYvx%2FvIjxuKAqDYThASJ8N3HZX1%2Fimg.png)

<br/>

## 5. Nesterov Accelerated Gradient(NAG)

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FQRlGZ%2FbtqVIJJ5eV7%2F41dCpKtKZT3vEJ1kiVpZiK%2Fimg.png)

<br/>

Momentumì´ ë„ˆë¬´ í° ê²½ìš°, ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì€ local minimaë¥¼ ì§€ë‚˜ì³ ê³„ì† ì»¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•˜ì—¬ Nesterov Accelerated Gradient(NAG)ê°€ ë“±ì¥í•˜ì˜€ìŠµë‹ˆë‹¤. Nesterov Accelerated Gradient(NAG)ëŠ” ë¯¸ë˜ë¥¼ ë³´ê³  í˜„ì¬ì˜ ê´€ì„±ì„ ì¡°ì ˆí•˜ì—¬ ì—…ë°ì´íŠ¸í•˜ëŠ” optimizerì…ë‹ˆë‹¤. ê·¸ë˜ì„œ local minimumì„ ë†“ì¹˜ì§€ ì•Šê³  ì†ë„ë¥¼ ëŠ¦ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, Momentumì˜ ë¹ ë¥¸ ì´ë™ê³¼ ì ì ˆí•œ ì‹œì ì— ì œë™ì„ ê±¸ ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë³€ìˆ˜ë¥¼ ì—¬ì „íˆ ìˆ˜ë™ì ìœ¼ë¡œ ì„ íƒí•˜ëŠ” ë‹¨ì ë„ ìˆìŠµë‹ˆë‹¤.

<br/>

## 6\. Adaptive Gradient(Adagrad)

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FMkAJj%2FbtqVIbGPy89%2FLe2WcP3lkjO3pM03GpyK11%2Fimg.png)

<br/>

AdagradëŠ” ë³€ìˆ˜ì˜ ì—…ë°ì´íŠ¸ íšŸìˆ˜ì— ë”°ë¼ í•™ìŠµë¥ (learning rate)ì„ ì¡°ì ˆí•˜ëŠ” ì˜µì…˜ì´ ì¶”ê°€ëœ ìµœì í™” ë°©ë²•ì…ë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ì„¤ëª…í•œ optimizerë“¤ì˜ ë‹¨ì ì€ ëª¨ë“  cycle ë° ë³€ìˆ˜ë“¤ì— ëŒ€í•˜ì—¬ í•™ìŠµë¥ (learning rate)ê°€ ì¼ì •í•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ AdagradëŠ” í•™ìŠµë¥ (learning rate)ì— ë³€í™”ë¥¼ ì¤ë‹ˆë‹¤. AdagardëŠ” ë§ì´ ë³€í™”í•˜ì§€ ì•Šì€ ë³€ìˆ˜ë“¤ì€ í•™ìŠµë¥ (learning rate)ì„ í¬ê²Œ í•˜ê³ , ë°˜ëŒ€ë¡œ ë§ì´ ë³€í™”í•œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ì„œëŠ” í•™ìŠµë¥ (learning rate)ì„ ì ê²Œ í•©ë‹ˆë‹¤. ì´ëŠ” ë§ì´ ë³€í™”í•œ ë³€ìˆ˜ëŠ” ìµœì ê°’ì— ê·¼ì ‘í•˜ì˜€ì„ ê²ƒì´ë¼ëŠ” ê°€ì • í•˜ì— ì‘ì€ í¬ê¸°ë¡œ ì´ë™í•˜ë©´ì„œ ì„¸ë°€í•œ ê°’ì„ ì¡°ì •í•˜ê³ , ë°˜ëŒ€ë¡œ ì ê²Œ ë³€í™”í•œ ë³€ìˆ˜ë“¤ì€ í•™ìŠµë¥ (learning rate)ì„ í¬ê²Œ í•˜ì—¬ ë¹ ë¥´ê²Œ ì†ì‹¤(loss) ê°’ì„ ì¤„ì…ë‹ˆë‹¤. ì´ì²˜ëŸ¼ í•™ìŠµë¥ (learning rate)ì— ë³€í™”ë¥¼ ì¤€ë‹¤ëŠ” ì ê³¼ ìˆ˜ë™ì ìœ¼ë¡œ í•™ìŠµë¥ (learning rate)ì„ ì¡°ì •í•  í•„ìš”ê°€ ì—†ë‹¤ëŠ” ì¥ì ì´ ìˆì§€ë§Œ, í•™ìŠµì´ ë„ˆë¬´ ë¹ ë¥´ê²Œ ëŠë ¤ì ¸ì„œ global minimumì— ìˆ˜ë ´í•˜ì§€ ëª»í•˜ëŠ” ë‹¨ì ë„ ì¡´ì¬í•©ë‹ˆë‹¤.

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcVMDKF%2FbtqVIJ4psUx%2FCAJx1EBhTkFw5CP3nvb9Mk%2Fimg.png)

<br/>

## 7\. Root Mean Square PROPagation(RMSprop)

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fb0Utbz%2FbtqVQXN1dRP%2FAnZt6s47en5NsriGS6qqSK%2Fimg.png)

<br/>

Adagardì˜ ë¬¸ì œì ì„ ê°œì„ í•˜ê¸° ìœ„í•˜ì—¬ ê°€ì¥ ìµœê·¼ ë°˜ë³µì—ì„œ ë¹„ë¡¯ëœ ê¸°ìš¸ê¸°(gredient)ë§Œ ëˆ„ì ì‹œí‚¤ëŠ” optimizerì…ë‹ˆë‹¤. Adagradì™€ì˜ ì°¨ì´ëŠ” ê°ë§ˆê°€ ìˆëŠëƒ ì—†ëŠëƒì…ë‹ˆë‹¤. ì´ëŠ” ì´ì „ì˜ ê¸°ìš¸ê¸°ì™€ í˜„ì¬ì˜ ê¸°ìš¸ê¸°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. $G$ ê³„ì‚°ì‹ì— ì§€ìˆ˜ì´ë™í‰ê· ì„ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. í•™ìŠµì´ ì§„í–‰ë¨ì— ë”°ë¼ parameter ì‚¬ì´ ì°¨ë³„í™”ëŠ” ìœ ì§€í•˜ë˜ í•™ìŠµì†ë„ê°€ ì§€ì†ì ìœ¼ë¡œ ì¤„ì–´ë“¤ì–´ 0ì— ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbIi1Yq%2FbtqVSuR8Xlo%2FzMZVGYzw41zpAsIXo9heuk%2Fimg.png)

<br/>

## 8. Adaptive Delta(AdaDelta)

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FdJZNvD%2FbtqVK7X1Fpj%2F3XkXh8mGmGDb5WwLgl5kl0%2Fimg.png)

<br/>

RMSpropì™€ ë§ˆì°¬ê°€ì§€ë¡œ Adagradì˜ í•™ìŠµë¥ ì´ ë–¨ì–´ì§€ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê°œë°œëœ optimizerì…ë‹ˆë‹¤. ì´ì „ì— ì œê³±ëœ ëª¨ë“  gradientë¥¼ ëˆ„ì í•˜ëŠ” ëŒ€ì‹  AdaDeltaëŠ” ëˆ„ì ëœ ì´ì „ì˜ ê¸°ìš¸ê¸°(gradient)ì˜ ì°½ì„ ê³ ì •ëœ í¬ê¸°(window size: w)ë¡œ ì œí•œí•˜ê³  ì§€ìˆ˜ì´ë™í‰ê· ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. í•™ìŠµë¥ ì´ ë–¨ì–´ì§€ì§€ ì•Šê¸° ë•Œë¬¸ì— í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê³„ì‚°ì†ë„ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦½ë‹ˆë‹¤.

<br/>

## 9. Adaptive moment esimation(Adam)

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FqOfgn%2FbtqVOqQx6iT%2FBiWopD5t0Lyv1EPrxR5xYk%2Fimg.png)

<br/>

Adamì€ ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ë©°, Momentumê³¼ RMSpropì´ ê²°í•©í•˜ì—¬ ìµœì†Ÿê°’ì„ ë›°ì–´ë„˜ì§€ ì•Šë„ë¡ ì†ë„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ê°œë°œëœ optimizerì…ë‹ˆë‹¤. Momentumì—ì„œ ê´€ì„±ê³„ìˆ˜ $\\gamma$ê³¼ í•¨ê»˜ ê³„ì‚°ëœ $V{t}$ë¡œ parameterë¥¼ ì—…ë°ì´íŠ¸í•˜ì§€ë§Œ ê¸°ìš¸ê¸° ê°’ê³¼ ê¸°ìš¸ê¸°ì˜ ì œê³± ê°’ì˜ ì§€ìˆ˜ì´ë™í‰ê· ì„ í™œìš©í•˜ì—¬ step ë³€í™”ëŸ‰ì„ ì¡°ì ˆí•©ë‹ˆë‹¤. Momentumì²˜ëŸ¼ ì´ì „ì˜ gradient ì§€ìˆ˜ ê°ì†Œ í‰ê· ì„ ë”°ë¥´ê³  RMSpropì²˜ëŸ¼ ì´ì „ì˜ gredient ì œê³±ì˜ ì§€ìˆ˜ ê°ì†Œëœ í‰ê· ì„ ë”°ë¦…ë‹ˆë‹¤. Adamì€ ë§¤ìš° ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ì§€ë§Œ, ê³„ì‚°ì†ë„ëŠ” ëŠë¦½ë‹ˆë‹¤.

<br/>

![image](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FIURQ8%2FbtqVK8P76mq%2FQUAEOb0HVeDTrNptOrv440%2Fimg.png)

<br/>

### ğŸ¤ ë§ˆì¹˜ë©°

ì´ìƒ ìµœì í™” ì•Œê³ ë¦¬ì¦˜(Optimizer)ì— ëŒ€í•´ ê°„ëµíˆ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤.
ë‹¤ë“¤ ê¸´ ê¸€ ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•˜ê³  ë‹¤ìŒ í¬ìŠ¤íŠ¸ë„ ê¸°ëŒ€í•´ì£¼ì„¸ìš”~ ğŸ‰

<br/>

### Reference

[https://blog.naver.com/i\_am\_sangyun/222165187257](https://blog.naver.com/i_am_sangyun/222165187257)

[https://blog.naver.com/stu5073/222159804653](https://blog.naver.com/stu5073/222159804653)

[https://blog.naver.com/winddori2002/221957205824](https://blog.naver.com/winddori2002/221957205824)

[https://blog.naver.com/youjaeah/222218660537](https://blog.naver.com/youjaeah/222218660537)

[https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam](https://onevision.tistory.com/entry/Optimizer-%EC%9D%98-%EC%A2%85%EB%A5%98%EC%99%80-%ED%8A%B9%EC%84%B1-Momentum-RMSProp-Adam)

ê¹€ìˆ˜ìœ¤(Sooyoon Kim), ì •ìš°ê·¼(Wookeen Chung), ì‹ ì„±ë ¬(Sungryul Shin). 2019. "Adam Optimizerë¥¼ ì´ìš©í•œ ìŒí–¥ë§¤ì§ˆ íƒ„ì„±íŒŒ ì™„ì „íŒŒí˜•ì—­ì‚°." ì§€êµ¬ë¬¼ë¦¬ì™€ ë¬¼ë¦¬íƒì‚¬, 22(4) : 202-209

ë§ˆìƒ¤ ëª¨ë¼ë””(Mahsa Moradi), ì´íƒœì‚¼(Taesam Lee). 2018. "Comparison of Optimization Algorithms in Deep Learning-Based Neural Networks for Hydrological Forecasting: Case Study of Nam River Daily Runoff." í•œêµ­ë°©ì¬í•™íšŒë…¼ë¬¸ì§‘, 18(6) : 377-384



<br/>

### ê¸€ì“´ì´

DSC Yonsei ì˜¤ì„¸ë¹ˆ

E-mail: [osb3372@yonsei.ac.kr](http://osb3372@yonsei.ac.kr)
